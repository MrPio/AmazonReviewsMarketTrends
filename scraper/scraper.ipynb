{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Amazon Reviews\n",
    "This notebook can be used to scrape reviews of a product on Amazon.\n",
    "\n",
    "> [!CAUTION]\n",
    "> This code is intended for **educational and research purposes only**. The use of this code to scrape Amazon may violate their [Terms of Service](https://www.amazon.com/gp/help/customer/display.html?nodeId=508088) and could lead to legal consequences.\n",
    "> By using this script, you agree that you understand these warnings and disclaimers.\n",
    "\n",
    "## Getting the URL\n",
    "The `get_url` function returns the URL of a page containing $10$ reviews of the given product with the given number of stars. \n",
    "\n",
    "> [!NOTE]\n",
    "> This script was tested to work on January 16, 2025. As you know, web scraping depends on the website, which evolves over time. In the future, `get_url` may not return the correct URL.\n",
    "\n",
    "With a given filter configuration, Amazon limits the number of pages to $10. So I decided to filter by the number of stars. If available, this script will collect 500 reviews, 100 1-star reviews, 100 2-star reviews, and so on.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Amazon requires the user to be logged to view the reviews dedicated page. Therefore, you need to login with your browser and export your cookies, as a JSON file in the `/scraper` directory. I used [*Cookie-Editor*](https://cookie-editor.com/) to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_NAME = 'HP-OfficeJet-Printer-Thermal-Inkjet'\n",
    "PRODUCT_ID = 'B08XW7LT9P'\n",
    "\n",
    "\n",
    "def get_url(product_name: str = PRODUCT_NAME, product_id: str = PRODUCT_ID, pageNumber: int = 1, stars:int=1) -> str:\n",
    "    star_conversion={1:'one',2:'two',3:'three',4:'four',5:'five'}\n",
    "    return f\"https://www.amazon.com/{product_name}/product-reviews/{product_id}/ref=cm_cr_getr_d_paging_btm_next_{pageNumber}?ie=UTF8&reviewerType=all_reviews&pageNumber={pageNumber}&sortBy=helpful&filterByStar={star_conversion[stars]}_star\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cookies for the login\n",
    "cookies = {}\n",
    "with open('cookies.json') as f:\n",
    "    cookies = {cookie['name']: cookie['value'] for cookie in json.load(f)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set headers to mimick a Firefox client\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0\",\n",
    "}\n",
    "\n",
    "# Make the requests\n",
    "stars = list(range(1, 6))\n",
    "pages = list(range(1, 11))\n",
    "# random.shuffle(pages)\n",
    "for star in stars:\n",
    "    for page in pages[:]:\n",
    "        reviews_raw[f'{star}:{page}'] = requests.get(get_url(pageNumber=page, stars=star),\n",
    "                                         cookies=cookies, headers=headers).text\n",
    "\n",
    "# Save the scraping results\n",
    "with open('reviews_raw.json', 'w') as f:\n",
    "    json.dump(reviews_raw, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
